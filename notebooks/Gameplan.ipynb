{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Training using ensemble networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks classifying together to ensure robustness by correcting each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the research question at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate do artificial neural networks have to be,\n",
    "to efficiently train each other in an image recognition task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What technical Background needs to be established?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feed Forward Neural Networks\n",
    "    - Perceptron\n",
    "    - Layers\n",
    "    - Activation functions\n",
    "    - Gradient Descent\n",
    "    - Optimizers\n",
    "- Deep learning\n",
    "- Convolutional Neural Networks\n",
    "    - Convolution\n",
    "    - Pooling\n",
    "- Ensemble Networks\n",
    "- Transfer Learning\n",
    "- Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Ensemble Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check feasibility of the method by looking at the case-probability distribution\n",
    "    - the difference between 3 and 5 network ensemble\n",
    "        - 5's postrain is more skewed\n",
    "        - 3's dont have the case of no prediction\n",
    "        \n",
    "    - discuss the 5 networks total-case-plot\n",
    "        - from which point on is method effective: ~70%\n",
    "            - at this point the case of no prediction is still high\n",
    "            - indicates a lot of hand labelling which is fine\n",
    "            - very good networks on complex tasks with a lot of classes may only have 80 % accuracy\n",
    "            \n",
    "        - most postrain data collected: ~80%\n",
    "            - Because no prediction is far less common and negtrain & allwrong fade out\n",
    "            \n",
    "        - end game: >90%\n",
    "            - postrain declines due to exponential growth of allright\n",
    "            - chance of no prediction below 10%\n",
    "            - postrain chance at 95% acc is still 20% while noprediction is 2%\n",
    "            - chances of negtrain & allwrong below 0.5%\n",
    "            \n",
    "    - Looks good on paper Lets see how this translates into the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Presentation of the hard facts of the dataset\n",
    "- The easiest to present our method\n",
    "- Executed preprocessing\n",
    "- Choosen augmentation for the simulation of a changing real world environment\n",
    "- Different test datasets generators for the different stages of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre training\n",
    "- majority vote\n",
    "- Cycle outline\n",
    "   - small learning rate\n",
    "       - halved compared to normal training\n",
    "       \n",
    "   - batch size 1\n",
    "       - stochastic gradient descent when data is heterogenous\n",
    "       - especially in the end game the continuous training data consists of edge cases that are not well represented by averaging over their gradients. They have to be individually represented\n",
    "       \n",
    "   - datapoints per cycle\n",
    "       - Depends on use case. How often is it even possible to retrain?\n",
    "       - in mid game a lot of data is missed. Many per cycles means a lot of hand labour.\n",
    "       - in end game datapoints per cycle can be higher due to less missed data\n",
    "       - Maybe introduce a threshold of data collected that start continuous training\n",
    "       \n",
    "- Interesting cases to look at\n",
    "    - running 100000 datapoints at different accs\n",
    "        - and check that with the theoretical framework\n",
    "        \n",
    "    - slight augmentation.\n",
    "        - how many cycles until ensemble adapted?\n",
    "        \n",
    "    - Hard augmentation\n",
    "        - How many cycles until ensemble adapted?\n",
    "        \n",
    "    - Plot rotation x cycles till adaption:\n",
    "        - How does the relation look like?\n",
    "        - Is there an optimal point?\n",
    "        \n",
    "    - step wise increased augmentation\n",
    "        - Check if ensemble accuracy is stable\n",
    "        - the influence of datapoints per cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nachweisliche steigerung der ensemble accuracy durch die methode\n",
    "- Präsentation des verlaufs der klassenaccuracies über die cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mit verweis auf die obrigen visualisierungen der klassenaccuracies aufklären wie die stärken der jeweiligen netzwerke sich gegenseitig balancieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nur letzte layer nachtrainieren, vor allem bei großeren Netzwerken.\n",
    "- Die implementierung eines threshold unter dem der output eines netzwerkes eine extra klasse die unsicherheit ausdrückt ist. In diesem Fall, würde das Netzwerk nachtrainiert. Die soll eine Entscheidung, die auf einer Mehrzahl von low probability predictions getätigt wird, verhindern.\n",
    "- Dahingehend, kann man auch ein average aus den predictions ziehen und alle unter dem threshold nachtrainieren.\n",
    "- Wenn eine Entscheidung kollektiv mit niedriger wahrscheinlichkeit getätigt wurde, sollte diese auch händisch geprüft werden.\n",
    "- Generell fehlt auch noch die anwendung des händischen prüfens und die auswertung wie sehr dies den trainingsverlauf beeinflusst.\n",
    "- Der vergleich der anpassungsfähigkeit des ensembles bei einer stetig steigenden augmentation und einer harten augmentation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNNtjlEikE9m+npsusDBi5H",
   "collapsed_sections": [],
   "name": "Continuous Training in Ensemble Neural Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
